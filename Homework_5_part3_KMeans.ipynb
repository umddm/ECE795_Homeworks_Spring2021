{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework_4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sq8U3BtmhtRx"
      },
      "source": [
        "\n",
        "# ECE795 Advanced Big Data Analytics \n",
        "## Homework_5_part3 (Due 4/1, 30 Total Points)\n",
        "\n",
        "Set up Pyspark Environment.\n",
        "\n",
        "Tips for Colab:\n",
        "\n",
        "1. You will be disconnected if you are idle for more than 90 minutes and will be mandatorily disconnected after 12 hour connection. \n",
        "\n",
        "2. Once you got disconnected, you need to execute the codes from the beginning to setup the environment again.\n",
        "\n",
        "3. For the purpose of homework, it should be sufficient since each problem should not take more than 5 minutes to generate the results.\n",
        "\n",
        "4. To facilitate the use of Colab, you can use \"MainMenu - Runtime - Run all” to run all the cells in the notebook. So you do not have to click each cell to setup the environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lh5NCoc8fsSO"
      },
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-2.4.7/spark-2.4.7-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.7-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.7-bin-hadoop2.7\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIy1IL965Ogm"
      },
      "source": [
        "## Please implement the K-Means algorithm described in the lecture slide. And test the implemented algorithm with data in \"s1.txt\" (k=15). (30 Points)\n",
        "\n",
        "\n",
        "The coordinates of the center change less than 0.1 are considered as unchanged.\n",
        "\n",
        "### Input data format:\n",
        "Each row describes a point using two integers, which are its x and y coordinates.\n",
        "The ID of the point is the row number, i.e., the point in the first row has ID=1, the point in the second row has ID=2, etc.\n",
        "\n",
        "### Expected output format:\n",
        "Each line describes a cluster including the cluster ID and the coordinate of its center\n",
        "```\n",
        "Cluster 1 (12.00, 67.33)\n",
        "Cluster 2 (33.25, 23.02)\n",
        "...\n",
        "Cluster 15 (123.43, 456.30)\n",
        "```\n",
        "\n",
        "### Hint:\n",
        "`zipWithIndex()` can be used to add index to the rdd. Detailed description can be found at https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.zipWithIndex"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FrRFR9pVQ8L"
      },
      "source": [
        "# Please download the data using the following commands\n",
        "!wget http://cs.joensuu.fi/sipu/datasets/s1.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OmbLiCQ5ctu"
      },
      "source": [
        "#Question_4\n",
        "\n",
        "#Fill out here"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}