{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework_3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sq8U3BtmhtRx"
      },
      "source": [
        "\n",
        "# ECE795 Advanced Big Data Analytics Homework 3 (Due 2/25)\n",
        "\n",
        "Set up Pyspark Environment.\n",
        "\n",
        "Tips for Colab:\n",
        "\n",
        "1. You will be disconnected if you are idle for more than 90 minutes and will be mandatorily disconnected after 12 hour connection. \n",
        "\n",
        "2. Once you got disconnected, you need to execute the codes from the beginning to setup the environment again.\n",
        "\n",
        "3. For the purpose of homework, it should be sufficient since each problem should not take more than 5 minutes to generate the results.\n",
        "\n",
        "4. To facilitate the use of Colab, you can use \"MainMenu - Runtime - Run all” to run all the cells in the notebook. So you do not have to click each cell to setup the environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lh5NCoc8fsSO"
      },
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-2.4.7/spark-2.4.7-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.7-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.7-bin-hadoop2.7\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEb4HTRwiaJx"
      },
      "source": [
        "Congrats! Your Colab is ready to run Pyspark.\n",
        "\n",
        "# Read input text file to RDD \n",
        "\n",
        "Download the input data from using the following command."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAISFqHXf7dt"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/umddm/ECE795_Homeworks_Spring2021/homework_1/Gutenberg.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21D9EANUvnwF"
      },
      "source": [
        "Now that we have input data, we can start to do the homework. \n",
        "\n",
        "## Question 1 (10 points): The following RDD, 'words', is obtained from Homework 2, Question 3. It contains all the words in the text file, 'Gutenberg.txt', after pre-processing. Using this RDD, please create another RDD called 'frequencies' that contains the term frequencies of all words.\n",
        "\n",
        "### Example output from 'frequencies.take(10)':\n",
        "```\n",
        "[('project', 95),\n",
        " ('ten', 208),\n",
        " ('use', 35),\n",
        " ('anyone', 5),\n",
        " ('anywhere', 6),\n",
        " ('united', 20),\n",
        " ('world', 39),\n",
        " ('whatsoever', 2),\n",
        " ('may', 84),\n",
        " ('give', 155)]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZeJ7WQCgM8g"
      },
      "source": [
        "#Question_1:\n",
        "from pyspark import SparkConf, SparkContext\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('words')\n",
        "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "english_words = set(nltk.corpus.words.words())\n",
        "\n",
        "def removeNonAlpabet(s):\n",
        "    return ''.join([i.lower() for i in s if i.isalpha() or i==' ']).lstrip().rstrip()\n",
        "\n",
        "sc = SparkContext.getOrCreate()\n",
        "sentences = sc.textFile('Gutenberg.txt').map(removeNonAlpabet).filter(lambda x: x!='')\n",
        "words = sentences.flatMap(lambda x: x.split()).filter(lambda x: x in english_words and x not in stop_words)\n",
        "\n",
        "#Fill out here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3vYyp5dwOm_"
      },
      "source": [
        "## Question 2 (10 points): For the 'frequencies' RDD in Question 1, normalize the values of term frequencies so that its sum is equal to 1 (also referred to as L1-normalization).\n",
        "\n",
        "The L1-normalized term frequency of the $n^{th}$ word can be calculated as:\n",
        "\n",
        "## $\\left\\Vert f_n \\right\\Vert_1 = \\frac{f_n}{f_1+f_2+\\dots+f_N}$\n",
        "\n",
        "where $f_n$ and $\\left\\Vert f_n \\right\\Vert_1$ are the term frequencies of the $n^{th}$ word before and after L1-normalization; $N$ is the total number of words in 'frequencies' RDD; $f_1, f_2, \\dots ,f_N$ are the term frequencies of the $1^{st}, 2^{nd}, \\dots , N^{th}$ words before normalization.\n",
        "\n",
        "### Example output of L1_frequencies.take(10):\n",
        "\n",
        "```\n",
        "[('project', 0.0007253071103001244),\n",
        " ('ten', 0.001588040830972904),\n",
        " ('use', 0.00026721840905794057),\n",
        " ('anyone', 3.817405843684865e-05),\n",
        " ('anywhere', 4.5808870124218384e-05),\n",
        " ('united', 0.0001526962337473946),\n",
        " ('world', 0.0002977576558074195),\n",
        " ('whatsoever', 1.526962337473946e-05),\n",
        " ('may', 0.0006413241817390574),\n",
        " ('give', 0.0011833958115423082)]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eja1BLiaTThT"
      },
      "source": [
        "#Question_2\n",
        "\n",
        "#Fill out here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FHVHVQjd4P1"
      },
      "source": [
        "## Question 3 (10 points): For the 'frequencies' RDD in Question 1, normalize the values of term frequencies so that the sum their square values is equal to 1 (also referred to as L2-normalization).\n",
        "\n",
        "The L2-normalized term frequency of the $n^{th}$ word can be calculated as:\n",
        "\n",
        "## $\\left\\Vert f_n \\right\\Vert_2 = \\frac{f_n}{\\sqrt{f^2_1+f^2_2+\\dots+f^2_N} }$\n",
        "\n",
        "where $f_n$ and $\\left\\Vert f_n \\right\\Vert_2$ are the term frequencies of the $n^{th}$ word before and after L2-normalization; $N$ is the total number of words in 'frequencies' RDD; $f^2_1, f^2_2, \\dots ,f^2_N$ are the squares of term frequencies of the $1^{st}, 2^{nd}, \\dots , N^{th}$ words before normalization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJsxix2Xd3-Q"
      },
      "source": [
        "#Question_3\n",
        "import numpy as np # To perform the square root operation, use np.sqrt()\n",
        "\n",
        "#Fill out here"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}