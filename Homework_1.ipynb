{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sq8U3BtmhtRx"
      },
      "source": [
        "\n",
        "# **Running Pyspark in Colab**\n",
        "PySpark is the Python API for Spark, which is an important tool in Big Data. To run spark in Colab, we need to first install all the dependencies in Colab environment i.e. Apache Spark 2.4.7 with hadoop 2.7, Java 8 and Findspark to locate the spark in the system. The tools installation can be carried out inside the Jupyter Notebook of the Colab. Follow the steps to install the dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lh5NCoc8fsSO"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-2.4.7/spark-2.4.7-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.7-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILheUROOhprv"
      },
      "source": [
        "Now that you installed Spark and Java in Colab, it is time to set the environment path which enables you to run Pyspark in your Colab environment. Set the location of Java and Spark by running the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1b8k_OVf2QF"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.7-bin-hadoop2.7\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwrqMk3HiMiE"
      },
      "source": [
        "Run a local spark session to test your installation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_Uz1NL4gHFx"
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEb4HTRwiaJx"
      },
      "source": [
        "Congrats! Your Colab is ready to run Pyspark.\n",
        "\n",
        "# Read input text file to RDD \n",
        "\n",
        "The first step to use Pyspark is reading input text file to resilient distributed dataset (RDD) provides by Spark as the primary abstraction.\n",
        "\n",
        "Download the input data from [here](https://raw.githubusercontent.com/umddm/ECE795_Homeworks/master/clust_B.txt) and keep it in the Colab document by the following command. The input data can also be downloaded by the link."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAISFqHXf7dt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4679b9b-c77b-4d2d-c55b-478ceb33ecc7"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/umddm/ECE795_Homeworks/master/clust_B.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-04 00:39:31--  https://raw.githubusercontent.com/umddm/ECE795_Homeworks/master/clust_B.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2021-02-04 00:39:31 ERROR 404: Not Found.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNsM_jHqrjBg"
      },
      "source": [
        "Check the input data correctly in the system by the following command"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m606eNuQgA82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81347baa-4ab9-4321-e098-76f08ababc33"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gutenberg.txt  spark-2.4.7-bin-hadoop2.7\n",
            "sample_data    spark-2.4.7-bin-hadoop2.7.tgz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21D9EANUvnwF"
      },
      "source": [
        "Now that we have input data, we can start to do the homework. \n",
        "\n",
        "## Question 1: (a) use \"sc.textFile\" command to read the provided text file, \"Gutenberg.txt\", as an RDD; (b) use \"flatMap\" command to split each sentence into seperate words; (c) use \"take\" command to observe its first 10 elements.\n",
        "\n",
        "### Expected output of rdd.take(10):\n",
        "```\n",
        "['The',\n",
        " 'Project',\n",
        " 'Gutenberg',\n",
        " 'eBook',\n",
        " 'of',\n",
        " 'Engelsch',\n",
        " 'woordenboek,',\n",
        " 'by',\n",
        " 'K.',\n",
        " 'ten']\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZeJ7WQCgM8g"
      },
      "source": [
        "#sc.textFile\n",
        "from pyspark import SparkConf, SparkContext\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "#Question_1:\n",
        "\n",
        "#Fill out here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3vYyp5dwOm_"
      },
      "source": [
        "## Question 2: For the RDD in Question 1, (a) use \"map\" command and the following \"removeNonAlpabet\" function to remove all non-alpahbetic characters in each word; (b) use \"take\" command to observe its first 10 elements.\n",
        "\n",
        "Please do not upload the generated output files to Blackboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eja1BLiaTThT"
      },
      "source": [
        "#Question_2\n",
        "# The following function will remove all non-alphabetic characters in a string\n",
        "# e.g. The output of removeNonAlpabet('A012,b') will be 'ab'\n",
        "def removeNonAlpabet(s):\n",
        "  return ''.join([i.lower() for i in s if i.isalpha()])\n",
        "\n",
        "#Fill out here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FHVHVQjd4P1"
      },
      "source": [
        "## Question 3: For the RDD in Question 2, please count the frequencies (numbers of occurences) of words (a) 'gutenberg', (b) 'press', and (c) 'printer', seperately."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJsxix2Xd3-Q"
      },
      "source": [
        "#Question_3\n",
        "\n",
        "#Fill out here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIy1IL965Ogm"
      },
      "source": [
        "## Question 4: For the RDD in Question 2, please compute the average length of all the words.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OmbLiCQ5ctu"
      },
      "source": [
        "#Question_4\n",
        "\n",
        "#Fill out here"
      ],
      "execution_count": 1,
      "outputs": []
    }
  ]
}