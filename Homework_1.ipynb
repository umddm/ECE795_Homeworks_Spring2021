{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sq8U3BtmhtRx"
      },
      "source": [
        "# **Running Pyspark in Colab** (Due date: February 11 at 1 p.m.)\n",
        "PySpark is the Python API for Spark, which is an important tool in Big Data. To run spark in Colab, we need to first install all the dependencies in Colab environment i.e. Apache Spark 2.4.7 with hadoop 2.7, Java 8 and Findspark to locate the spark in the system. The tools installation can be carried out inside the Jupyter Notebook of the Colab. Follow the steps to install the dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lh5NCoc8fsSO"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-2.4.7/spark-2.4.7-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.7-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILheUROOhprv"
      },
      "source": [
        "Now that you installed Spark and Java in Colab, it is time to set the environment path which enables you to run Pyspark in your Colab environment. Set the location of Java and Spark by running the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1b8k_OVf2QF"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.7-bin-hadoop2.7\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwrqMk3HiMiE"
      },
      "source": [
        "Run a local spark session to test your installation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_Uz1NL4gHFx"
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEb4HTRwiaJx"
      },
      "source": [
        "Congrats! Your Colab is ready to run Pyspark.\n",
        "\n",
        "# Read input text file to RDD \n",
        "\n",
        "The first step to use Pyspark is reading input text file to resilient distributed dataset (RDD) provides by Spark as the primary abstraction.\n",
        "\n",
        "Download the input data from [here](https://github.com/umddm/ECE795_Homeworks_Spring2021/blob/homework_1/Gutenberg.txt) and keep it in the Colab document by the following command. The input data can also be downloaded by the link."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAISFqHXf7dt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdce3ee6-3794-4068-83e3-8da3e56cd5f7"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/umddm/ECE795_Homeworks_Spring2021/homework_1/Gutenberg.txt"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-04 03:35:04--  https://raw.githubusercontent.com/umddm/ECE795_Homeworks_Spring2021/homework_1/Gutenberg.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3615589 (3.4M) [text/plain]\n",
            "Saving to: ‘Gutenberg.txt’\n",
            "\n",
            "Gutenberg.txt       100%[===================>]   3.45M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2021-02-04 03:35:04 (26.5 MB/s) - ‘Gutenberg.txt’ saved [3615589/3615589]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNsM_jHqrjBg"
      },
      "source": [
        "Check the input data correctly in the system by the following command"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m606eNuQgA82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a37d7983-3ca6-4a2a-858b-6bd129644eab"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gutenberg.txt  spark-2.4.7-bin-hadoop2.7\n",
            "sample_data    spark-2.4.7-bin-hadoop2.7.tgz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21D9EANUvnwF"
      },
      "source": [
        "Now that we have input data, we can start to do the homework. \n",
        "\n",
        "## (5 points) Question 1: (b) load the provided text file, \"Gutenberg.txt\", into an RDD; (b) observe its first 10 elements.\n",
        "\n",
        "### Expected output:\n",
        "```\n",
        "['The Project Gutenberg eBook of Engelsch woordenboek, by K. ten Bruggencate',\n",
        " '',\n",
        " 'This eBook is for the use of anyone anywhere in the United States and',\n",
        " 'most other parts of the world at no cost and with almost no restrictions',\n",
        " 'whatsoever. You may copy it, give it away or re-use it under the terms',\n",
        " 'of the Project Gutenberg License included with this eBook or online at',\n",
        " 'www.gutenberg.org. If you are not located in the United States, you',\n",
        " 'will have to check the laws of the country where you are located before',\n",
        " 'using this eBook.',\n",
        " '']\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZeJ7WQCgM8g"
      },
      "source": [
        "#sc.textFile\n",
        "from pyspark import SparkConf, SparkContext\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "#Question_1:\n",
        "\n",
        "#Fill out here"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3vYyp5dwOm_"
      },
      "source": [
        "## (5 points) Question 2: For the RDD in Question 1, (a) use the following \"removeNonAlpabet\" function to remove all non-alpahbetic characters and convert all alphabets into lower case; (b) remove the empty sentences (empty string \" \") from the RDD\n",
        "\n",
        "Please do not upload the generated output files to Blackboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eja1BLiaTThT"
      },
      "source": [
        "#Question_2\n",
        "# The following function will remove all non-alphabetic characters in a sentence and convert all alphabets into lower case\n",
        "# e.g. The output of removeNonAlpabet('A012,b') will be 'ab'\n",
        "def removeNonAlpabet(s):\n",
        "  return ''.join([i.lower() for i in s if i.isalpha() or i==' ']).lstrip().rstrip()\n",
        "\n",
        "#Fill out here"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FHVHVQjd4P1"
      },
      "source": [
        "## (5 points) Question 3: For the RDD in Question 2, please count the numbers of sentences that contain the following words (a) 'gutenberg', (b) 'press', and (c) 'printer', seperately."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJsxix2Xd3-Q"
      },
      "source": [
        "#Question_3\n",
        "\n",
        "#Fill out here"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIy1IL965Ogm"
      },
      "source": [
        "## (5 points) Question 4: For the RDD in Question 2, please compute the average number of words in a sentence.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OmbLiCQ5ctu"
      },
      "source": [
        "#Question_4\n",
        "\n",
        "#Fill out here"
      ],
      "execution_count": 53,
      "outputs": []
    }
  ]
}